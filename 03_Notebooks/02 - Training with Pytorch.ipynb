{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Deploying our ML Model\n",
    "\n",
    "**SageMaker Studio Kernel**: Data Science\n",
    "\n",
    "In this exercise you will do:\n",
    " - Upload the dataset\n",
    " - Train your ML model using Pytorch\n",
    " - Compute the thresholds, used by the application, to classify the predictions as anomalies or normal behavior\n",
    " - Compile/Optimize your model to your edge device (Linux ARM64) using [SageMaker NEO](https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html)\n",
    " - Create a deployment package with a signed model + the runtime used by SageMaker Edge Agent to load and invoke the optimized model\n",
    " - Deploy the package using IoT Jobs\n",
    "\n",
    "\n",
    "### Upload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import glob\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(role)\n",
    "sagemaker_session=sagemaker.Session()\n",
    "sagemaker_session\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "\n",
    "prefix='wind_turbine_anomaly'\n",
    "data_files = glob.glob('data/*.npy')\n",
    "\n",
    "train_input = \"s3://%s/%s/data\" % (bucket_name, prefix)\n",
    "\n",
    "for f in data_files:\n",
    "    sagemaker_session.upload_data(f, key_prefix=\"%s/data\" % prefix)\n",
    "n_features = np.load(data_files[0]).shape[1]\n",
    "\n",
    "print(train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add required permission\n",
    "\n",
    "In order to be able to create the components in AWS IoT Greengrass we need to add the following permissions to the above role:\n",
    "\n",
    "`greengrass:CreateComponentVersion`\n",
    "\n",
    "1. Open the IAM Console and search for the role printed above\n",
    "2. Click on **Attach inline policy**\n",
    "3. Select the JSON tab and enter\n",
    "   ```\n",
    "   {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "    {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"greengrass:CreateComponentVersion\", \"greengrass:DescribeComponent\"],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "    }\n",
    "    ```\n",
    "4. Click on **Review policy**\n",
    "5. Enter a name\n",
    "6. Click on **Create policy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wind_turbine.py\n",
    "import argparse\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from   torch.autograd import Variable\n",
    "from   sklearn.model_selection import KFold\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def create_model(n_features, dropout=0):    \n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(n_features, 32, kernel_size=2, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(dropout),\n",
    "        torch.nn.Conv2d(32, 64, kernel_size=2, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(dropout),\n",
    "        torch.nn.Conv2d(64, 128, kernel_size=2, padding=2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.ConvTranspose2d(128, 64, kernel_size=2, padding=2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(dropout),\n",
    "        torch.nn.ConvTranspose2d(64, 32, kernel_size=2, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(dropout),\n",
    "        torch.nn.ConvTranspose2d(32, n_features, kernel_size=2, padding=1),\n",
    "    )    \n",
    "\n",
    "def load_data(data_dir):\n",
    "    input_files = glob.glob(os.path.join(data_dir, '*.npy'))\n",
    "    data = [np.load(i) for i in input_files]\n",
    "    return np.vstack(data)    \n",
    "\n",
    "def train_epoch(optimizer, criterion, epoch, model, train_dataloader, test_dataloader):\n",
    "    train_loss = 0.0    \n",
    "    test_loss = 0.0    \n",
    "    model.train()\n",
    "    for x_train, y_train in train_dataloader:\n",
    "        # clearing the Gradients of the model parameters\n",
    "        optimizer.zero_grad()\n",
    "        # prediction for training and validation set        \n",
    "        output_train = model(x_train)        \n",
    "        loss_train = criterion(output_train, y_train)\n",
    "                \n",
    "        # computing the updated weights of all the model parameters\n",
    "        # statistics\n",
    "        train_loss += loss_train.item()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()        \n",
    "    model.eval()\n",
    "    for x_test, y_test in test_dataloader:            \n",
    "        output_test = model(x_test.float())\n",
    "        loss_test = criterion(output_test, y_test)\n",
    "        # statistics\n",
    "        test_loss += loss_test.item()                \n",
    "        \n",
    "    return train_loss, test_loss\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = torch.load(os.path.join(model_dir, \"model.pth\"))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def predict_fn(input_data, model):    \n",
    "    with torch.no_grad():\n",
    "        return model(input_data.float().to(device))\n",
    "\n",
    "def train(args):\n",
    "    best_of_the_best = (0,-1)\n",
    "    best_loss = 10000000\n",
    "    num_epochs = args.num_epochs\n",
    "    batch_size = args.batch_size    \n",
    "    \n",
    "    X = load_data(args.train)\n",
    "    criterion = nn.MSELoss()    \n",
    "    kf = KFold(n_splits=args.k_fold_splits, shuffle=True)\n",
    "    \n",
    "    for i, indexes in enumerate(kf.split(X)):\n",
    "        # skip other Ks if fixed was informed\n",
    "        if args.k_index_only >= 0 and args.k_index_only != i: continue\n",
    "        \n",
    "        train_index, test_index = indexes\n",
    "        print(\"Test dataset proportion: %.02f%%\" % (len(test_index)/len(train_index) * 100))\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        X_train = torch.from_numpy(X_train).float().to(device)\n",
    "        X_test = torch.from_numpy(X_test).float().to(device)\n",
    "\n",
    "        train_dataset = torch.utils.data.TensorDataset(X_train, X_train)\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "        test_dataset = torch.utils.data.TensorDataset(X_test, X_test)\n",
    "        test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "        model = create_model(args.num_features, args.dropout_rate)\n",
    "        model = model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "        # Instantiate model\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            train_loss, test_loss = train_epoch( optimizer, criterion, epoch, model, train_dataloader, test_dataloader)\n",
    "            elapsed_time = (time.time() - start_time)\n",
    "            print(\"k=%d; epoch=%d; train_loss=%.3f; test_loss=%.3f; elapsed_time=%.3fs\" % (i, epoch, train_loss, test_loss, elapsed_time))\n",
    "            if test_loss < best_loss:                \n",
    "                torch.save(model.state_dict(), os.path.join(args.output_data_dir,'model_state.pth'))\n",
    "                best_loss = test_loss\n",
    "                if best_loss < best_of_the_best[0]:\n",
    "                    best_of_the_best = (best_loss, i)\n",
    "    print(\"\\nBest model: best_mse=%f;\" % best_loss)\n",
    "    model = create_model(args.num_features, args.dropout_rate)\n",
    "    model.load_state_dict( torch.load(os.path.join(args.output_data_dir, \"model_state.pth\")) )    \n",
    "    torch.save(model, os.path.join(args.model_dir, \"model.pth\"))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nn.DataParallel\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters are described here. In this simple example we are just including one hyperparameter.    \n",
    "    parser.add_argument('--k_fold_splits', type=int, default=6)\n",
    "    parser.add_argument('--k_index_only', type=int, default=-1)\n",
    "    parser.add_argument('--batch_size', type=int, default=16)\n",
    "    parser.add_argument('--num_epochs', type=int, default=10)\n",
    "    parser.add_argument('--num_features', type=int, default=8)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.003)\n",
    "    parser.add_argument('--dropout_rate', type=float, default=0.0)\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--num-gpus', type=int, default=os.environ['SM_NUM_GPUS'])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model with SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    'wind_turbine.py', \n",
    "    framework_version='1.6.0',\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    #instance_type='local_gpu',    \n",
    "    instance_count=1,\n",
    "    py_version='py3', \n",
    "    hyperparameters={\n",
    "        'k_fold_splits': 6,\n",
    "        'k_index_only': 3, # after running some experiments with this dataset, it makes sense to fix it\n",
    "        'num_epochs': 200,\n",
    "        'batch_size': 256,\n",
    "        'learning_rate': 0.0001,\n",
    "        'dropout_rate': 0.001,\n",
    "        'num_features': n_features\n",
    "    },\n",
    "    metric_definitions=[\n",
    "        {'Name': 'train_loss:mse', 'Regex': ' train_loss=(\\S+);'},\n",
    "        {'Name': 'test_loss:mse', 'Regex': ' test_loss=(\\S+);'}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'train': train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the threshold based on MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = estimator.transformer(\n",
    "    instance_count=1, \n",
    "    instance_type='ml.p2.xlarge', \n",
    "    output_path=\"s3://%s/%s/output\" % (bucket_name, prefix),\n",
    "    accept='application/x-npy',\n",
    "    max_payload=20,\n",
    "    strategy='MultiRecord',\n",
    "    assemble_with='Line'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To start a transform job:\n",
    "transformer.transform(train_input, content_type='application/x-npy')\n",
    "# Then wait until transform job is completed\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.download_data(bucket=bucket_name, key_prefix='wind_turbine_anomaly/output/', path='data/preds/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute MAE & the thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "x_inputs = np.vstack([np.load(i) for i in data_files])\n",
    "y_preds = np.vstack([np.load(i) for i in glob.glob('data/preds/*.out')])\n",
    "\n",
    "n_samples,n_features,n_rows,n_cols = x_inputs.shape\n",
    "\n",
    "x_inputs = x_inputs.reshape(n_samples, n_features, n_rows*n_cols).transpose((0,2,1))\n",
    "y_preds = y_preds.reshape(n_samples, n_features, n_rows*n_cols).transpose((0,2,1))\n",
    "\n",
    "mae_loss = np.mean(np.abs(y_preds - x_inputs), axis=1).transpose((1,0))\n",
    "mae_loss[np.isnan(mae_loss)] = 0\n",
    "\n",
    "thresholds = np.mean(mae_loss, axis=1)\n",
    "print(\",\".join(thresholds.astype(str)), thresholds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the trained model for Jetson Nano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ensure we have the latest version of boto3 installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "sm_client = boto3.client('sagemaker')\n",
    "compilation_job_name = 'wind-turbine-anomaly-%d' % int(time.time()*1000)\n",
    "sm_client.create_compilation_job(\n",
    "    CompilationJobName=compilation_job_name,\n",
    "    RoleArn=role,\n",
    "    InputConfig={\n",
    "        'S3Uri': '%s%s/output/model.tar.gz' % (estimator.output_path, estimator.latest_training_job.name),\n",
    "        'DataInputConfig': '{\"input0\":[1,%d,10,10]}' % n_features,\n",
    "        'Framework': 'PYTORCH'\n",
    "    },\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': 's3://%s/wind_turbine/optimized/' % sagemaker_session.default_bucket(),\n",
    "        'TargetPlatform': { 'Os': 'LINUX', 'Arch': 'ARM64', 'Accelerator': 'NVIDIA' },\n",
    "        'CompilerOptions': '{\"trt-ver\": \"7.1.3\", \"cuda-ver\": \"10.2\", \"gpu-code\": \"sm_53\"}' # Jetpack 4.4.1\n",
    "    },\n",
    "    StoppingCondition={ 'MaxRuntimeInSeconds': 900 }\n",
    ")\n",
    "while True:\n",
    "    resp = sm_client.describe_compilation_job(CompilationJobName=compilation_job_name)    \n",
    "    if resp['CompilationJobStatus'] in ['STARTING', 'INPROGRESS']:\n",
    "        print('Running...')\n",
    "    else:\n",
    "        print(resp['CompilationJobStatus'], compilation_job_name)\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Deployment Package including the Greengrass Componenent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the Greengrass component we add two new paramters to the `OutputConfig`: `PresetDeploymentType` and `PresetDeploymentConfig`. You need to increate the component version every time you run the packaging job (in the example below we use the `model_version` for traceability), since Greengrass components are immutable. Alternatively, you can delete the existing component version before re-running the packagin job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "model_version = '1.0.0'\n",
    "model_name = 'WindTurbineAnomalyDetection'\n",
    "edge_packaging_job_name='wind-turbine-anomaly-%d' % int(time.time()*1000)\n",
    "component_name = \"aws.samples.windturbine.model\"\n",
    "resp = sm_client.create_edge_packaging_job(\n",
    "    EdgePackagingJobName=edge_packaging_job_name,\n",
    "    CompilationJobName=compilation_job_name,\n",
    "    ModelName=model_name,\n",
    "    ModelVersion=model_version,\n",
    "    RoleArn=role,\n",
    "    OutputConfig={\n",
    "        \"PresetDeploymentType\": \"GreengrassV2Component\",\n",
    "        \"PresetDeploymentConfig\": json.dumps({\"ComponentName\":component_name, \"ComponentVersion\": model_version}),\n",
    "        'S3OutputLocation': 's3://%s/%s/model' % (bucket_name, prefix)\n",
    "    }\n",
    ")\n",
    "while True:\n",
    "    resp = sm_client.describe_edge_packaging_job(EdgePackagingJobName=edge_packaging_job_name)    \n",
    "    if resp['EdgePackagingJobStatus'] in ['STARTING', 'INPROGRESS']:\n",
    "        print('Running...')\n",
    "    else:\n",
    "        print(resp['EdgePackagingJobStatus'], compilation_job_name)\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the packaging job completed, not all packaging artifacts might have been created. Run the following to check if the Greengrass component has been created. If not, check the `StatusMessage` to understand the reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.describe_edge_packaging_job(EdgePackagingJobName=edge_packaging_job_name)['PresetDeploymentOutput']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the component to Greengrass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now deploy the Component to Greengrass by following the [README](../04_EdgeApplication/README.md). \n",
    "\n",
    "Once the deployment has been performed, you can observe the results of the inference by using the [MQTT test console](https://console.aws.amazon.com/iot/home?#/test) and subscribe to `wind-turbines/logs/#` topic.\n",
    "\n",
    "Execute the following cell and take note of the S3 bucket name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}